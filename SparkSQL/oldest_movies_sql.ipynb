{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already satisfied: pyspark==2.4.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (2.4.5)\r\nRequirement already satisfied: py4j==0.10.7 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pyspark==2.4.5) (0.10.7)\r\n"
                }
            ],
            "source": "!pip install pyspark==2.4.5"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SQLContext, SparkSession\nfrom pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n\nfrom ibm_botocore.client import Config\nimport ibm_boto3\nimport os\nfrom pyspark.sql.functions import unix_timestamp, to_date\nimport pyspark.sql.functions as F\n\n\nimport numpy as np\nimport pandas as pd\nimport json\nimport pyspark.sql\nfrom pyspark.sql.functions import col, skewness, kurtosis\nfrom pyspark.sql.functions import isnan, when, count, col\nfrom pyspark.sql.functions import when\nfrom pyspark.sql.functions import UserDefinedFunction\nfrom pyspark.sql.types import StringType\nfrom datetime import date, timedelta, datetime\nimport time\n\nsc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\nfrom pyspark.sql import SparkSession\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Hidden cell with credentials for accessing from Cloud Object Storage"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "from zipfile import ZipFile\n\ncos = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id=credentials['IBM_API_KEY_ID'],\n    ibm_service_instance_id=credentials['IAM_SERVICE_ID'],\n    ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n    config=Config(signature_version='oauth'),\n    endpoint_url=credentials['ENDPOINT'])\ncos.download_file(Bucket=credentials['BUCKET'],Key=credentials['FILE'][0],Filename=credentials['FILE'][0])\ncos.download_file(Bucket=credentials['BUCKET'],Key=credentials['FILE'][1],Filename=credentials['FILE'][1])\ncos.download_file(Bucket=credentials['BUCKET'],Key=credentials['FILE'][2],Filename=credentials['FILE'][2])\n\nwith ZipFile(credentials['FILE'][0],'r') as zipObj:\n    zipObj.extractall()"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "spark.read.text(\"movies.dat\").createOrReplaceTempView(\"movies_view\")\n\n## To use spark.sql, it should be at least a temporary view or even an table\nspark.sql(\"\"\" select\nsplit(value,'::')[0] as movieid,\nsplit(value,'::')[1] as moviename,\nsubstring(split(value,'::')[1],length(split(value,'::')[1])-4,4) as year\nfrom movies_view \"\"\").createOrReplaceTempView(\"movies\");\n\n## To view the records, use spark.sql(\"select * from movies\").show()\nresult=spark.sql(\"Select * from movies m1 where m1.year=(Select min(m2.year) from movies m2)\").repartition(1).rdd\n#to check: print(result.collect())\n\n##To save it on the distributed file system on cloud\nresult.saveAsTextFile(\"file:///home/username/result\")"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<a download=\"oldest_movies.csv\" href=\"data:text/csv;base64,bW92aWVpZCxtb3ZpZW5hbWUseWVhcgoyODIxLE1hbGUgYW5kIEZlbWFsZSAoMTkxOSksMTkxOQoyODIzLCJTcGlkZXJzLCBUaGUgKERpZSBTcGlubmVuLCAxLiBUZWlsOiBEZXIgR29sZGVuZSBTZWUpICgxOTE5KSIsMTkxOQozMTMyLERhZGR5IExvbmcgTGVncyAoMTkxOSksMTkxOQo=\" target=\"_blank\">Download CSV file</a>",
                        "text/plain": "<IPython.core.display.HTML object>"
                    },
                    "execution_count": 20,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "## To save it locally\nfrom IPython.display import HTML \nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"result.csv\"):  \n    csv = df.to_csv(sep=',',header=True, index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n\n# rdd to df\nresult_df = result.toDF().toPandas()\ncreate_download_link(result_df,filename='oldest_movies.csv')"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
